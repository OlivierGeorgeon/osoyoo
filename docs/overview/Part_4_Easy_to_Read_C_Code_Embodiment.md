
##



# PETITCAT PROJECT DISCLAIMER

**Date:** July 1, 2024

This is a public document that can be modified by others who have access. If anything doesn't make sense, please ignore it. Original contributors may not be aware of the full contents of this document or what is still current over time.

**Safety Notice:**
No use of high voltage or anything dangerous is or should be specified in this document. Any illustrations are or should be contributors' own, generated by ethical generative software and modified by themselves, or taken from open technical sites.

**Copyright and Fair Use:**
Reproduction is limited to "fair use" or otherwise not allowed if prohibited by law. Software, documentation, and logo belong to (c) and (tm) the PetitCat project and creators but are allowed for "fair use" or otherwise not allowed if prohibited by law. Usage is "as is"â€”users should consider and treat this project as experimental.

**Project Nature:**
This is a multiple-person open-source GitHub project. Note that this project is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the license indicated on the GitHub site for more information. If no license is specified, then by default the MIT License or the Apache-2 License applies:
- [MIT License](https://opensource.org/licenses/MIT)
- [Apache License](https://opensource.org/licenses/Apache-2.0)

**Content and Contributions:**
All content included in this project, such as code, documentation, and related materials, should be created by the contributors of this project or should be open-source software components. To the best of our knowledge, all content complies with copyright laws and is either original or used with permission. If any content should not be here, please advise, and a correction will be made in a suitable time period. Note that material in this project can be modified by members of the public once they join the project, and other members are not responsible for changes by one member.

**Patent Disclaimer:**
We, the contributors of this project, do not provide any warranty or guarantee that the use, distribution, or modification of this project does not infringe on any patents. It is the responsibility of the user or distributor of this project to ensure compliance with any applicable patent laws. This project should be used for non-commercial purposes only.

**Liability:**
By using this project, you agree that you are solely responsible for any legal issues or liabilities that may arise. The contributors of this project shall not be held liable for any direct, indirect, incidental, or consequential damages arising from the use, distribution, or modification of this project. Do NOT use this project for any mission-critical (including any health) projects.

**Contributions:**
If you contribute to this project, you assert that your contributions are given freely and are your original work and do not violate any third-party copyrights, patents, or other intellectual property rights. You grant permission for your contributions to be used in this project under the project's open-source license.

**Contact:**
If any content should not be here or if there are any legal concerns, please advise, and a correction will be made in a suitable time period.


##



# An Easy-to-Read Overview of the Robot Car ("PetitCat") Project
# Part IV

![petitcatgpt4logo](petitcatgpt4logo.jpg)
-

-

<h2 style="font-size: 24px;">The PetitCat Project </h2>


---------------

Step by step explanations for beginners and researchers alike. The documentation is intended to give any user a pleasant experience with the project regardless of how seriously they intend to make use of the project. An issue for many researchers in using GitHub software is that it more often fails to work (or work properly) mainly because the thousands of litte things in the heads of the developers are not made clear to the non-involved user. Here we have gone to the other extreme, to make sure that the hardware and software will work for any user, although the more advanced topics are geared towards researchers rather than students. We have paid much attention to the main causes of poor GitHub and other open source software usability: incomplete documentation, dependency issues, environment configuration, version mismatches, non-graceful error handling, permissions and access, network/connectivity issues, stability, indadequate testing, user prerequisite knowledge. Easy-to-read, guaranteed-to-work and inexpensive may surprise you in producing an example of a superhuman AGI with robotic embodiment.

----------------


The "Easy-to-Read Overview" of the project is divided into a number of parts:

**Part I:** The Basics: Assembly, Software and Using

**Part II:** Modifying the Robot Car for Python Control

**Part III:** Interfacing your Python Code with the PetitCat Project

**Part IV:** Modifying the C/C++ for your Selected Robotic Embodiment

**Part V:** Integration of PetitCat with a Causal Cognitive Architecture

**Part VI:** Integration of PetitCat with a Large Language Model

**Part VII:** Active Inference of the Robot Car


The PetitCat documentation is written so that anyone with a basic education can read it and understand it. Having a deeper breadth of knowledge in software development, AI, or cognitive science, can, of course, allow greater appreciation for certain aspects of the project.

-Part I does not require any specialized background knowledge. You should be familiar with moving files around in either your Windows, Mac or Linux desktop/laptop computer. Other than that, no specialized knoweldge is needed. Part I is perfectly fine for high school students and hobbyists.

-Part II may be fine for high school students, college students and hobbyists as well, given a willingness of some effort to learn things here and there. However, the documentation will guide you on this learning journey. The lower level software of the PetitCat project is written in C\C++ in the Arduino IDE. You don't need to know these languages or environment in order to use Python with the project. However, we provide a very basic C\C++\Arduino tutorial which may be sufficient for most readers who do want to have a bit more control over the Arduino board coding. We just touch upon the Python control of the project at the end of Part II, so you are not expected to have much Python knowledge in this part, unless you want to start modifying the Python files.

-Part III is where the PetitCat project becomes more useful -- interfacing the Python code of your AI/AGI project with the PetitCat projects.

-Part IV allows you to modify the Arduino C/C++ code to use other robotic embodiments than the default robot car, or to add addtional sensors and actuators to the robot car.

-In Parts V and VI we will integrate the PetitCat project with a cognitive architecture and then a large language model. Again, in this part, only intermediate (or even novice) Python coding abilities are required. However, we will gently guide you through the project, so that you end up with a super-human intelligent grounded autonomous robot system.

-Part VII really only requires some knowledge of Python. You do not have to be an expert developer. More important, is perhaps a background in AI or cognitive science. However, there is no real background prerequisite here. Regardless of the reader's background, we provide gentle tutorials on a number of topics, and guide the learner through the concepts of active inference and implementing it in the PetitCat project.

-
-

# Part IV: Modifying the C/C++ for your Selected Robotic Embodiment

-
-



-

<h1 style="font-size: 24px;">Step #1 -- </h1>

-

Pending documentation....

-



<p align="center">
 <img src="horsetonextlesson.png" width="770" height="770">
</p>



-
-
-
-


.....

<img src="rewrite.png" width="370" height="370">

# thinking..... writing..... thinking..... writing....


-
-
-

![oliviermotorwiring](oliviermotorwiring.jpg)
-



 We will also further modify the robot car, starting with changing the wiring of the servomechanism shortly below. Then we will go on to further modify our robot car with a digital compass, inertial measurement unit and a color sensor. Then we will consider the Python code controlling all this. If you don't know how to code in Python there are many excellent free courses available and you can still do this project. (However, I am assuming anyone doing this project and wanting to interface real-world hardware with their Python code, knows how to code the latter.) Then we will move on to consider Active Inference and how the robot car demonstrates this important principle. 

You made it this far, and you will succeed again in this part.  Get on that horse. Saddle up!!  






-

<h1 style="font-size: 24px;">Step #2 -- Rewiring the ServoMechanism</h1>



## The head servo is overheating



## Optionally change the servo

The servo `LACC200610` provided by Osoyoo was jittering when the wheels were moving. I replaced it with the `SG90` servo from my Elegoo kit, which solved the problem.


# Modify some wiring

We have modified some wiring from the original osoyoo robot as listed in Table 1.

Table 1: Wiring modification

|Original |Modified ||
|---|---|---|
|5|23| Rear  Right Motor direction pin 1|
|6|25| Rear  Right Motor direction pin 2|
|13|6|Head servo pin|

Pins 5 and 6 were moved to 23 and 25 because they don't need the PWM functionality. These pins are declared in the file `Wheel.h`.

Pin 13 was moved to pin 6 because we are using the arduino onboard LED that is also connected to Pin 13. The head servo pin is declared in the file `Robot_define.h`.

# Connect the IMU card

Connect the IMU card to the wifi Shield following Table 2 and Figure 1. 
Note that the calibration offset varies significantly depending on the position of the card on the robot. 

Table 2: IMU Wiring

|GY-86 Imu|MEGA2560 Wifi Shield|
|---|---|
|Vcc|3v3|
|GND|GND|
|SCL|SCL|
|SDA|SDA|

![image](/docs/wiki/assets/gy86wiring.jpg)
Figure 1: wiring of GY-86 imu

# Connect the color sensor

Use the "long" two-hole version of the TCS34725. It has two LEDs, and the holes miraculously match holes on the robot platform (Figure 4).
Wire it to the Arduino MEGA according to Table 3. 
Use PINs 20 and 21 for SDA and SCL because the other SDA and SCL PINs are used for the IMU. They are the same. 
For the power line, I used the 3v yellow PINs on the Wifi Shield. 
Alternatively, the VIN PIN can be connected to a 5V PIN (red).

![image](/docs/wiki/assets/TCS34725.jpg)
Figure 4: TCS34725 color sensor mounted on two 40 mm pillars between the front wheel motors

![image](/docs/wiki/assets/TCS34725(2).jpg)
Figure 5: TCS34725 color sensor to Arduino MEGA wiring

Table 3: TCS34725 PIN connection

|TCS34725 |MEGA2560 ||
|---|---|---|
|3v3|3v| On the Wifi Shield, yellow slots|
|GND|GND| On the Wifi Shield, black|
|SDA|SDA 20| |
|SCL|SCL 21| |
|LED|53|Digital output|

More information on the [TCS34725 adafruit webpage](https://learn.adafruit.com/adafruit-color-sensors/overview).

# The emotion LED

Install a common cathode RGB LED as shown below. 

![image](/docs/wiki/assets/rgbled.png)

Figure 5: RGB LED with flat side on the left. The cathode is the longest lead.

Table 4: RGB LED connections

|RGB LED | |MEGA2560 / Wifi shield|
|---|---|---|
|Blue|| 2 |
|Green|| 3 |
|GND|10kÎ© resistor| GND |
|Red|| 5 |

![image](/docs/wiki/assets/rgbled(2).jpg)

.....

.....


,# Center the head

Place an object in front of the robot. Press `"-"` to remote control the robot to perform a scan and align its head towards the object.
Switch off the robot.
Tighten the head on the servo axis such that the angle between the head direction and the robot x axis correspond to the value `"head_angle"` in the `outcome` packet (Figure 2). Positive angles correspond to the robot looking to the left (trigonometric direction). `0` is centered. 

# Calibrate the floor luminosity sensors

Follow instructions in [Lesson 3](https://osoyoo.com/2022/07/05/v2-metal-chassis-mecanum-wheel-robotic-for-arduino-mega2560-lesson3-5-point-line-tracking/). The adjustment of the potentiometer can be quite sensitive. It may be helpful to place the robot on shims to raise it by a few millimeters to prevent it from moving when it detects the black line.

# Calibrate the floor color sensor

Place the robot on a white sheet of paper. 
Press `"-"` to remote control the robot to perform a scan while staying in place. 
In `RobotDefine.h`, set the values of `WHITE_RED`, `WHITE_GREEN`, and `WHITE_BLUE` respectively to the values of `"red"`, `"green"`, and `"blue"` in the `outcome` packet read in the terminal.

# Calibrate the compass

The Body Memory Window (Figure 1) shows the compass points representing where the robot believed the south was at each of the last 10 steps. 
Large blue squares are compass points relative to the robot's orientation. 
As the robot turns around, they draw a circle around the robot. The better this circle is centered on the robot, the better the compass is calibrated. 
Small blue squares are compass points in an absolute position. The closest they are to each other, the better the compass and the gyroscope are calibrated.

![image](/docs/wiki/assets/bodymemoryview.png)

_Figure 1: Body Memory View. Large blue squares forming a circle: estimated positions of the south on each step relative to the robot's orientation. Small blue squares at the bottom: estimated positions of the south on each step in absolute position._

To calibrate the compass, remote control the robot to make it turn at least 8 times of 45Â°.

If you can see the circle of blue squares (you may need to zoom out) in Body Memory Window: 
* Select the Body Memory window and press `"O"`. This will center the circle of compass points and display the message `Compass offset adjusted by (x, y)` at the bottom of the window.  
* Add `"x"` to `COMPASS_X_OFFSET` and `"y"` to `COMPASS_Y_OFFSET` in `Robot_define.h`.
* Download the Arduino code to the robot and relaunch the PC application.

If you cannot see the circle of blue squares in Body Memory Window, or if pressing `O` returns an error message, then it means that the offset is too far off.
* Use the values of `"compass_x"` and `"compass_y"` (Figure 2) in the `Outcome` packet as initial values of `COMPASS_X_OFFSET` and `COMPASS_Y_OFFSET`, and redo the procedure

# Check the gyroscope calibration

The file `Robot_define.h` contains a parameter `GYRO_COEF` used to scale the gyroscope measure. 
Good news: the value `1` seems to work for all robots, no need to calibrate!

The PC application computes the azimuth of the robot as the average between (1) the azimuth measured by the compass and (2) the estimated azimuth obtained by adding the yaw measured by the gyroscope to the previous azimuth. 
The Body Memory window displays the delta between these two values. 
On average, we observe a difference on the order of 1 or 2 degrees, which is fine.  

# Calibrate the linear accelerometer

We use the linear accelerometer to detect impacts with obstacles. 
Figure 2 shows an example `Outcome` packet received by the PC from the robot. 
The non-nul `impact` field indicates that the robot detected an impact.

![image](/docs/wiki/assets/terminalwindow.png)

_Figure 2: The Terminal window in PyCharm. The outcome packet contains the fields `impact`, `max_x_acc`, and `min_x_acc` used to calibrate the accelerometer._

## Calibrate the linear accelerometer offset

To calibrate the linear accelerometer `x offset`:
* Place the robot on shims so that the wheels don't touch the floor and the robot won't move. Make sure it is horizontal to prevent the gravitational acceleration to affect the x acceleration measure.
* Press `8` and `2` to remote control the robot to move forward and backward several times. Since it can't move, the robot may stop immediately because it believes that it is blocked by an obstacle. 
* In the terminal, find the `max_x_acc` and `min_x_acc` in the `Outcome` packet. 
The `min_x_acc` should be negative and corresponds to a maximum backwards acceleration. 
* Add `(max_x_acc + min_x_acc)/2` to `ACCELERATION_X_OFFSET` in `Robot_define.h`. 
For example, with the values obtained in Figure 2, the value `(-46-66)/2=-56` must be added to `ACCELERATION_X_OFFSET`.
Compute the average over several steps. 
Try to obtain an offset value error better than +/-30.

Proceed similarly to calibrate the linear accelerometer `y offset`. 
Download the Arduino code to the robot.

## Calibrate the impact threshold

The accelerometer x impact threshold is the threshold of deceleration beyond which an impact outcome is triggered. 
To calibrate this threshold:
* Remote control the robot to move forward and backward and to bump into an obstacle.
* In the Terminal window, find the `max_x_acc` and `min_x_acc` in the `Outcome` packet. 
Now these values incorporate the offset defined previously.
* When the robot bumps into an obstacle while moving forward, the strong deceleration causes `min_x_acc` to have a high negative value. 
The `impact` outcome is triggered when `min_x_acc < -ACCELERATION_X_IMPACT_THRESHOLD`.
* When the robot bumps into an obstacle while moving backward, the strong deceleration causes `max_x_acc` to have a high positive value. 
The `impact` outcome is triggered when `max_x_acc > ACCELERATION_X_IMPACT_THRESHOLD`.
* In `Robot_define.h` increase `ACCELERATION_X_IMPACT_THRESHOLD` if the `impact` outcome is triggered when there is no impact. Decrease it if the `impact` outcome is not triggered when there is an impact.

Proceed similarly for the accelerometer y impact threshold.
Download the Arduino code to the robot.

## Calibrate the accelerometer x block threshold

The accelerometer x block threshold is the threshold beneath which the impact outcome is triggered if the robot cannot accelerate because it is blocked by an obstacle. To calibrate this threshold:
* Place the robot against an obstacle.
* Remote control the robot to try to move forward.
* In the Terminal window, find the `max_x_acc` and `min_x_acc` in the `Outcome` packet. 
* When the robot tries to move forward but is blocked by an obstacle, the x acceleration will remain low. 
The `impact` outcome is triggered when `max_x_acc < ACCELERATION_X_BLOCK_THRESHOLD` during the initial 100 milliseconds.
* When the robot tries to move backward but is blocked by an obstacle, the x acceleration will remain at a low negative value. 
The `impact` outcome is triggered when `min_x_acc > -ACCELERATION_X_BLOCK_THRESHOLD` during the initial 100 milliseconds.
* In `Robot_define.h` increase `ACCELERATION_X_BLOCK_THRESHOLD` if the `impact` outcome is triggered when the robot is not blocked by an obstacle. Decrease it if the `impact` outcome is not triggered when the robot is blocked by an obstacle.

Proceed similarly for the accelerometer y block threshold.
Download the Arduino code to the robot.


-
-
-
# Troubleshooting

## The head servo is overheating

This may be because you left the servo head connected to Pin 13.
Please make sure you moved it to Pin 6 as explained in [Modify some wiring](Assemby-the-robot.md#modify-some-wiring).

Pin 13 is used to control the built-in LED of the Arduino board. 
The petitcat arduino code sends a fast on/off signal to Pin 13 to make the LED blink quickly to indicate that the robot is waiting for a wifi command. 
If Pin 13 is connected to the head servo, the servo won't move but will overheat. 

## The robot moves backward

In some versions of the Osoyoo robot, the wheel wiring is backward. 
You may invert the wheel wiring or set negative coefficients to wheels in `robot_define.h` like this:

```
#define REAR_RIGHT_WHEEL_COEF -1
#define REAR_LEFT_WHEEL_COEF -1
#define FRONT_RIGHT_WHEEL_COEF -1
#define FRONT_LEFT_WHEEL_COEF -1
```

## Bad detection of objects by the HC-SR04 ultrasonic module

Bad reception of echo signals may result in: 
- PetitCat often fails to detect object.
- The outcome packet often misses the `"echo_distance"` field.
- After a "Scan" interaction, the `"echos"` field of the outcome packet contains no or few values.  

This may be caused by a poor electrical connection of the HC-SR04 ultrasonic module. 
Please check and tighten its wiring according to the diagram in [Osoyoo Lesson 2](https://osoyoo.com/2022/07/05/v2-mecanum-wheel-metal-chassis-robotic-for-arduino-mega2560-lesson-2-obstacle-avoidance-robot/).



****

(This is a public document that can be modified by others who have access -- if anything doesn't make sense, please ignore.)
(No use of high voltage or anything dangerous is or should be specified in this document.)

-
-


<p align="center">
 <img src="altlogo.png" width="470" height="470">
</p>

-
-

# Piloting or simulating PetitCat

Run `main.py` on a PC to open the windows shown in Figure 1. For example:

```
py -m main chezOlivier 1
```

![image](/docs/wiki/assets/gui.png)
Figure 1: Graphic User Interface

## Remote control the robot

Select any window of the PetitCat project and 
Press the control keys:

* `8` Move forward
* `2` Backward
* `4` Swipe left
* `6` Swipe right
* `1` Turn right
* `3` Turn left
* `-` Scan

The simulator shows the movement in the windows as the robot moves.
When the PC recieves the outcome from the robot, the views are updated to match the actual outcome.
The PC does not receive the outcome after a timeout, it resent the command. 
To stop the PC from resending the command, press `C` (Clear).

 
## Simulator

To enter simulator mode, select the window `Body memory` and press key `I` (Imagine).
The window displays `| Imaginary |`
Press the control keys. The commands will be simulated but not sent to the robot.


-
-


end of document
****
##





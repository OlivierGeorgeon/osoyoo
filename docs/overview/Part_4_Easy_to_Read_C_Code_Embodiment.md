
##

Initial draft -----, 2024

(This is a public document that can be modified by others who have access -- if anything doesn't make sense, please ignore. A year from now, for example, I may not be aware of the full contents of this document any longer or what is still current.) (No use of high voltage or anything dangerous is or should be specified in this document.) (Any illustrations are my own, generated by OpenAI GPT4 and modified by myself, or taken from open technical sites. Reproduction is limited for "fair use" or else not allowed if prohibited by law. Software/documentation/logo belongs/(c)/tm the PetitCat project/creators but is allowed for "fair use" or else not allowed if prohibited by law. Usage is "as is" -- user should consider and treat as experimental.)

(Questions, corrections, suggestions? -- please contact me: hschneidermd@alum.mit.edu  
Howard Schneider -- Feb 15, 2024)
##



# An Easy-to-Read Overview of the Robot Car ("PetitCat") Project
# Part IV

![petitcatgpt4logo](petitcatgpt4logo.jpg)
-

-

<h2 style="font-size: 24px;">The PetitCat Project </h2>




The "Easy-to-Read Overview" of the project is divided into a number of parts:

**Part I:** The Basics: Assembly, Software and Using

**Part II:** Modifying the Robot Car for Python Control

**Part III:** Interfacing your Python Code with the PetitCat Project

**Part IV:** Modifying the C/C++ for your Selected Robotic Embodiment

**Part V:** Integration of PetitCat with a Causal Cognitive Architecture

**Part VI:** Integration of PetitCat with a Large Language Model

**Part VII:** Active Inference of the Robot Car


The PetitCat documentation is written so that anyone with a basic education can read it and understand it. Having a deeper breadth of knowledge in software development, AI, or cognitive science, can, of course, allow greater appreciation for certain aspects of the project.

-Part I does not require any specialized background knowledge. You should be familiar with moving files around in either your Windows, Mac or Linux desktop/laptop computer. Other than that, no specialized knoweldge is needed. Part I is perfectly fine for high school students and hobbyists.

-Part II may be fine for high school students, college students and hobbyists as well, given a willingness of some effort to learn things here and there. However, the documentation will guide you on this learning journey. The lower level software of the PetitCat project is written in C\C++ in the Arduino IDE. You don't need to know these languages or environment in order to use Python with the project. However, we provide a very basic C\C++\Arduino tutorial which may be sufficient for most readers who do want to have a bit more control over the Arduino board coding. We just touch upon the Python control of the project at the end of Part II, so you are not expected to have much Python knowledge in this part, unless you want to start modifying the Python files.

-Part III is where the PetitCat project becomes more useful -- interfacing the Python code of your AI/AGI project with the PetitCat projects.

-Part IV allows you to modify the Arduino C/C++ code to use other robotic embodiments than the default robot car, or to add addtional sensors and actuators to the robot car.

-In Part V and VI we will integrate the PetitCat project with a cognitive architecture and then a large language model. Again, in this part, only intermediate (or even novice) Python coding abilities are required. However, we will gently guide you through the project, so that you end up with a super-human intelligent grounded autonomous robot system.

-Part VII really only requires some knowledge of Python. You do not have to be an expert developer. More important, is perhaps a background in AI or cognitive science. However, there is no real background prerequisite here. Regardless of the reader's background, we provide gentle tutorials on a number of topics, and guide the learner through the concepts of active inference and implementing it in the PetitCat project.

-
-

# Part IV: Modifying the C/C++ for your Selected Robotic Embodiment

-
-



-

<h1 style="font-size: 24px;">Step #1 -- </h1>

-

Pending documentation....

-



<p align="center">
 <img src="horsetonextlesson.png" width="770" height="770">
</p>



-
-
-
-


.....

<img src="rewrite.png" width="370" height="370">

# thinking..... writing..... thinking..... writing....


-
-
-

![oliviermotorwiring](oliviermotorwiring.jpg)
-



 We will also further modify the robot car, starting with changing the wiring of the servomechanism shortly below. Then we will go on to further modify our robot car with a digital compass, inertial measurement unit and a color sensor. Then we will consider the Python code controlling all this. If you don't know how to code in Python there are many excellent free courses available and you can still do this project. (However, I am assuming anyone doing this project and wanting to interface real-world hardware with their Python code, knows how to code the latter.) Then we will move on to consider Active Inference and how the robot car demonstrates this important principle. 

You made it this far, and you will succeed again in this part.  Get on that horse. Saddle up!!  






-

<h1 style="font-size: 24px;">Step #2 -- Rewiring the ServoMechanism</h1>



## The head servo is overheating



## Optionally change the servo

The servo `LACC200610` provided by Osoyoo was jittering when the wheels were moving. I replaced it with the `SG90` servo from my Elegoo kit, which solved the problem.


# Modify some wiring

We have modified some wiring from the original osoyoo robot as listed in Table 1.

Table 1: Wiring modification

|Original |Modified ||
|---|---|---|
|5|23| Rear  Right Motor direction pin 1|
|6|25| Rear  Right Motor direction pin 2|
|13|6|Head servo pin|

Pins 5 and 6 were moved to 23 and 25 because they don't need the PWM functionality. These pins are declared in the file `Wheel.h`.

Pin 13 was moved to pin 6 because we are using the arduino onboard LED that is also connected to Pin 13. The head servo pin is declared in the file `Robot_define.h`.

# Connect the IMU card

Connect the IMU card to the wifi Shield following Table 2 and Figure 1. 
Note that the calibration offset varies significantly depending on the position of the card on the robot. 

Table 2: IMU Wiring

|GY-86 Imu|MEGA2560 Wifi Shield|
|---|---|
|Vcc|3v3|
|GND|GND|
|SCL|SCL|
|SDA|SDA|

![image](/docs/wiki/assets/gy86wiring.jpg)
Figure 1: wiring of GY-86 imu

# Connect the color sensor

Use the "long" two-hole version of the TCS34725. It has two LEDs, and the holes miraculously match holes on the robot platform (Figure 4).
Wire it to the Arduino MEGA according to Table 3. 
Use PINs 20 and 21 for SDA and SCL because the other SDA and SCL PINs are used for the IMU. They are the same. 
For the power line, I used the 3v yellow PINs on the Wifi Shield. 
Alternatively, the VIN PIN can be connected to a 5V PIN (red).

![image](/docs/wiki/assets/TCS34725.jpg)
Figure 4: TCS34725 color sensor mounted on two 40 mm pillars between the front wheel motors

![image](/docs/wiki/assets/TCS34725(2).jpg)
Figure 5: TCS34725 color sensor to Arduino MEGA wiring

Table 3: TCS34725 PIN connection

|TCS34725 |MEGA2560 ||
|---|---|---|
|3v3|3v| On the Wifi Shield, yellow slots|
|GND|GND| On the Wifi Shield, black|
|SDA|SDA 20| |
|SCL|SCL 21| |
|LED|53|Digital output|

More information on the [TCS34725 adafruit webpage](https://learn.adafruit.com/adafruit-color-sensors/overview).

# The emotion LED

Install a common cathode RGB LED as shown below. 

![image](/docs/wiki/assets/rgbled.png)

Figure 5: RGB LED with flat side on the left. The cathode is the longest lead.

Table 4: RGB LED connections

|RGB LED | |MEGA2560 / Wifi shield|
|---|---|---|
|Blue|| 2 |
|Green|| 3 |
|GND|10kΩ resistor| GND |
|Red|| 5 |

![image](/docs/wiki/assets/rgbled(2).jpg)

.....

.....


,# Center the head

Place an object in front of the robot. Press `"-"` to remote control the robot to perform a scan and align its head towards the object.
Switch off the robot.
Tighten the head on the servo axis such that the angle between the head direction and the robot x axis correspond to the value `"head_angle"` in the `outcome` packet (Figure 2). Positive angles correspond to the robot looking to the left (trigonometric direction). `0` is centered. 

# Calibrate the floor luminosity sensors

Follow instructions in [Lesson 3](https://osoyoo.com/2022/07/05/v2-metal-chassis-mecanum-wheel-robotic-for-arduino-mega2560-lesson3-5-point-line-tracking/). The adjustment of the potentiometer can be quite sensitive. It may be helpful to place the robot on shims to raise it by a few millimeters to prevent it from moving when it detects the black line.

# Calibrate the floor color sensor

Place the robot on a white sheet of paper. 
Press `"-"` to remote control the robot to perform a scan while staying in place. 
In `RobotDefine.h`, set the values of `WHITE_RED`, `WHITE_GREEN`, and `WHITE_BLUE` respectively to the values of `"red"`, `"green"`, and `"blue"` in the `outcome` packet read in the terminal.

# Calibrate the compass

The Body Memory Window (Figure 1) shows the compass points representing where the robot believed the south was at each of the last 10 steps. 
Large blue squares are compass points relative to the robot's orientation. 
As the robot turns around, they draw a circle around the robot. The better this circle is centered on the robot, the better the compass is calibrated. 
Small blue squares are compass points in an absolute position. The closest they are to each other, the better the compass and the gyroscope are calibrated.

![image](/docs/wiki/assets/bodymemoryview.png)

_Figure 1: Body Memory View. Large blue squares forming a circle: estimated positions of the south on each step relative to the robot's orientation. Small blue squares at the bottom: estimated positions of the south on each step in absolute position._

To calibrate the compass, remote control the robot to make it turn at least 8 times of 45°.

If you can see the circle of blue squares (you may need to zoom out) in Body Memory Window: 
* Select the Body Memory window and press `"O"`. This will center the circle of compass points and display the message `Compass offset adjusted by (x, y)` at the bottom of the window.  
* Add `"x"` to `COMPASS_X_OFFSET` and `"y"` to `COMPASS_Y_OFFSET` in `Robot_define.h`.
* Download the Arduino code to the robot and relaunch the PC application.

If you cannot see the circle of blue squares in Body Memory Window, or if pressing `O` returns an error message, then it means that the offset is too far off.
* Use the values of `"compass_x"` and `"compass_y"` (Figure 2) in the `Outcome` packet as initial values of `COMPASS_X_OFFSET` and `COMPASS_Y_OFFSET`, and redo the procedure

# Check the gyroscope calibration

The file `Robot_define.h` contains a parameter `GYRO_COEF` used to scale the gyroscope measure. 
Good news: the value `1` seems to work for all robots, no need to calibrate!

The PC application computes the azimuth of the robot as the average between (1) the azimuth measured by the compass and (2) the estimated azimuth obtained by adding the yaw measured by the gyroscope to the previous azimuth. 
The Body Memory window displays the delta between these two values. 
On average, we observe a difference on the order of 1 or 2 degrees, which is fine.  

# Calibrate the linear accelerometer

We use the linear accelerometer to detect impacts with obstacles. 
Figure 2 shows an example `Outcome` packet received by the PC from the robot. 
The non-nul `impact` field indicates that the robot detected an impact.

![image](/docs/wiki/assets/terminalwindow.png)

_Figure 2: The Terminal window in PyCharm. The outcome packet contains the fields `impact`, `max_x_acc`, and `min_x_acc` used to calibrate the accelerometer._

## Calibrate the linear accelerometer offset

To calibrate the linear accelerometer `x offset`:
* Place the robot on shims so that the wheels don't touch the floor and the robot won't move. Make sure it is horizontal to prevent the gravitational acceleration to affect the x acceleration measure.
* Press `8` and `2` to remote control the robot to move forward and backward several times. Since it can't move, the robot may stop immediately because it believes that it is blocked by an obstacle. 
* In the terminal, find the `max_x_acc` and `min_x_acc` in the `Outcome` packet. 
The `min_x_acc` should be negative and corresponds to a maximum backwards acceleration. 
* Add `(max_x_acc + min_x_acc)/2` to `ACCELERATION_X_OFFSET` in `Robot_define.h`. 
For example, with the values obtained in Figure 2, the value `(-46-66)/2=-56` must be added to `ACCELERATION_X_OFFSET`.
Compute the average over several steps. 
Try to obtain an offset value error better than +/-30.

Proceed similarly to calibrate the linear accelerometer `y offset`. 
Download the Arduino code to the robot.

## Calibrate the impact threshold

The accelerometer x impact threshold is the threshold of deceleration beyond which an impact outcome is triggered. 
To calibrate this threshold:
* Remote control the robot to move forward and backward and to bump into an obstacle.
* In the Terminal window, find the `max_x_acc` and `min_x_acc` in the `Outcome` packet. 
Now these values incorporate the offset defined previously.
* When the robot bumps into an obstacle while moving forward, the strong deceleration causes `min_x_acc` to have a high negative value. 
The `impact` outcome is triggered when `min_x_acc < -ACCELERATION_X_IMPACT_THRESHOLD`.
* When the robot bumps into an obstacle while moving backward, the strong deceleration causes `max_x_acc` to have a high positive value. 
The `impact` outcome is triggered when `max_x_acc > ACCELERATION_X_IMPACT_THRESHOLD`.
* In `Robot_define.h` increase `ACCELERATION_X_IMPACT_THRESHOLD` if the `impact` outcome is triggered when there is no impact. Decrease it if the `impact` outcome is not triggered when there is an impact.

Proceed similarly for the accelerometer y impact threshold.
Download the Arduino code to the robot.

## Calibrate the accelerometer x block threshold

The accelerometer x block threshold is the threshold beneath which the impact outcome is triggered if the robot cannot accelerate because it is blocked by an obstacle. To calibrate this threshold:
* Place the robot against an obstacle.
* Remote control the robot to try to move forward.
* In the Terminal window, find the `max_x_acc` and `min_x_acc` in the `Outcome` packet. 
* When the robot tries to move forward but is blocked by an obstacle, the x acceleration will remain low. 
The `impact` outcome is triggered when `max_x_acc < ACCELERATION_X_BLOCK_THRESHOLD` during the initial 100 milliseconds.
* When the robot tries to move backward but is blocked by an obstacle, the x acceleration will remain at a low negative value. 
The `impact` outcome is triggered when `min_x_acc > -ACCELERATION_X_BLOCK_THRESHOLD` during the initial 100 milliseconds.
* In `Robot_define.h` increase `ACCELERATION_X_BLOCK_THRESHOLD` if the `impact` outcome is triggered when the robot is not blocked by an obstacle. Decrease it if the `impact` outcome is not triggered when the robot is blocked by an obstacle.

Proceed similarly for the accelerometer y block threshold.
Download the Arduino code to the robot.


-
-
-


****

(This is a public document that can be modified by others who have access -- if anything doesn't make sense, please ignore.)
(No use of high voltage or anything dangerous is or should be specified in this document.)

-
-


<p align="center">
 <img src="altlogo.png" width="470" height="470">
</p>


-
-


end of document
****
##




